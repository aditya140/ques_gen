{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "WARNING:torchtext.data.iterator:The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "a=dataset.load_dataset(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,    2,    2,    2,    2],\n",
      "        [   5,    5,   14,   14,  126],\n",
      "        [ 102,   27,   38,   38,   68],\n",
      "        [  12,  349,   24,  102,   10],\n",
      "        [  10,   15, 2401,   31,   26],\n",
      "        [ 194,  212,   32,    7,   51],\n",
      "        [  70,    4,   22,  251, 1916],\n",
      "        [  18,    3,  787,  101,  122],\n",
      "        [8309,    1,  884,   61,  219],\n",
      "        [   4,    1,   16,   23,   22],\n",
      "        [   3,    1,   12,    6,  125],\n",
      "        [   1,    1,    7, 6933,   20],\n",
      "        [   1,    1,   16,  119, 2312],\n",
      "        [   1,    1,  442,    4,  121],\n",
      "        [   1,    1,   32,    3,    4],\n",
      "        [   1,    1,   11,    1,    3],\n",
      "        [   1,    1,   26,    1,    1],\n",
      "        [   1,    1,  124,    1,    1],\n",
      "        [   1,    1,    8,    1,    1],\n",
      "        [   1,    1,  103,    1,    1],\n",
      "        [   1,    1,  179,    1,    1],\n",
      "        [   1,    1,   30,    1,    1],\n",
      "        [   1,    1, 3014,    1,    1],\n",
      "        [   1,    1,   15,    1,    1],\n",
      "        [   1,    1, 1045,    1,    1],\n",
      "        [   1,    1,    4,    1,    1],\n",
      "        [   1,    1,    3,    1,    1]])\n",
      "tensor([[   2,    2,    2,    2,    2],\n",
      "        [   6,    6,    6,    6,   49],\n",
      "        [  25,   25,   39,   39,   72],\n",
      "        [  12,   33,   13,   13,   14],\n",
      "        [   7,  330, 1030,   25,    8],\n",
      "        [ 218,    4,   32,   36,   31],\n",
      "        [ 168,  139,    7,   76,  171],\n",
      "        [   4,    5,    4,    7,   43],\n",
      "        [ 685,    3,  902,   31, 1201],\n",
      "        [  67,    1,  866,  156,   23],\n",
      "        [  27,    1,    8,  236,   10],\n",
      "        [1664,    1,   12,    4,  232],\n",
      "        [   5,    1,    7, 2450,    4],\n",
      "        [   3,    1,    8,  299,  169],\n",
      "        [   1,    1,  181,    5,    7],\n",
      "        [   1,    1,   10,    3,    8],\n",
      "        [   1,    1,   32,    1,  100],\n",
      "        [   1,    1,    9,    1,    5],\n",
      "        [   1,    1,    8,    1,    3],\n",
      "        [   1,    1,  262,    1,    1],\n",
      "        [   1,    1,   11,    1,    1],\n",
      "        [   1,    1,  601,    1,    1],\n",
      "        [   1,    1,   10,    1,    1],\n",
      "        [   1,    1,    4,    1,    1],\n",
      "        [   1,    1, 1428,    1,    1],\n",
      "        [   1,    1,   37,    1,    1],\n",
      "        [   1,    1,   50,    1,    1],\n",
      "        [   1,    1,   98,    1,    1],\n",
      "        [   1,    1,  146,    1,    1],\n",
      "        [   1,    1,    5,    1,    1],\n",
      "        [   1,    1,    3,    1,    1]])\n",
      "<torchtext.datasets.translation.Multi30k object at 0x7f15bef02c10>\n",
      "['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'src', 'trg', '__module__', '__doc__', '__init__', 'fromvars', '__repr__', '__str__', '__len__', '_get_field_values', '__iter__', '__dict__', '__weakref__', '__hash__', '__getattribute__', '__setattr__', '__delattr__', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__new__', '__reduce_ex__', '__reduce__', '__subclasshook__', '__init_subclass__', '__format__', '__sizeof__', '__dir__', '__class__']\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    for j in i:\n",
    "        print(j.src)\n",
    "        print(j.trg)\n",
    "        print(j.dataset)\n",
    "        print(j.__dir__())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tokenize_en(\"Hello hwoa are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator,TabularDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_lang = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>')\n",
    "opt_lang = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('ans', inp_lang), ('que', opt_lang)]\n",
    "train,val = TabularDataset.splits(path='./.data/', train='train.csv', validation='val.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_lang.build_vocab(train,val)\n",
    "opt_lang.build_vocab(train,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1969,  615, 3390, 3390, 2009, 2148, 1969]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(\".data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82767, 4)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.Vocab"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgen=dataset.QGenDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  43498\n",
      "Load Failure :  5945\n"
     ]
    }
   ],
   "source": [
    "qgen._fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.42B.300d.zip: 1.88GB [14:33, 2.15MB/s]                            \n",
      "100%|█████████▉| 1917372/1917494 [03:21<00:00, 10378.43it/s]"
     ]
    }
   ],
   "source": [
    "inp_lang.vocab.load_vectors(\"glove.42B.300d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.1457e-01, -5.1367e-01,  3.2210e-01,  5.4995e-02,  5.5951e-01,\n",
       "        -2.0149e-02, -1.6463e+00, -2.1642e-01, -1.8585e-01, -7.6916e-01,\n",
       "        -1.0133e-02, -6.2154e-02,  3.0545e-01, -3.9373e-01, -4.7257e-01,\n",
       "        -4.0850e-01,  1.1824e-01, -3.1163e-01, -5.0630e-03,  4.1088e-01,\n",
       "         3.3824e-01,  5.2521e-02,  3.0271e-01,  3.5258e-01, -1.7552e-01,\n",
       "        -4.7280e-04,  2.3839e-02,  4.5845e-01,  2.5750e-01,  3.3118e-02,\n",
       "         1.2671e-01, -1.9215e-01,  3.6240e-01, -7.1027e-02,  2.6915e-01,\n",
       "        -2.0077e-01,  7.4150e-02,  1.0814e-01,  2.2891e-01,  1.6823e-01,\n",
       "         1.1329e-01,  1.7791e-01,  2.3952e-02, -1.4993e-01, -1.2613e-01,\n",
       "        -9.8029e-02, -3.0535e-01, -9.4352e-02,  5.1821e-01, -4.3049e-01,\n",
       "        -2.8761e-01,  2.7622e-02,  2.0184e-01,  2.2188e-02,  1.4464e-01,\n",
       "         2.8736e-01,  3.3884e-01, -1.3414e-01,  9.7698e-01,  2.5356e-01,\n",
       "        -3.3928e-01,  1.4415e-03,  1.9131e-02, -3.9172e-01,  4.5803e-01,\n",
       "         5.1720e-01, -3.4131e-01,  6.8218e-02, -1.2524e-01,  7.9428e-02,\n",
       "         3.0650e-03,  4.0055e-01, -1.8837e-01,  3.9316e-01, -7.2131e-02,\n",
       "        -4.1886e-02,  1.8386e-01,  3.3677e-01,  1.7619e-01, -1.4437e-01,\n",
       "         5.4490e-01,  1.9301e-02, -3.1539e-01, -4.8676e-01,  2.0211e-01,\n",
       "         3.4774e-01,  3.0520e-01,  3.7232e-01, -3.1156e-01,  9.6372e-01,\n",
       "        -5.4087e-01, -1.3478e-01,  1.3174e-01, -2.3045e-01, -3.5977e-01,\n",
       "        -3.4047e-01, -8.5660e-01,  3.8469e-02,  1.5613e-01, -2.3571e-03,\n",
       "        -1.2716e-01, -1.5777e-01, -2.8656e-01, -3.3708e-01,  7.1274e-02,\n",
       "         2.5838e-01,  1.4784e-01, -6.3565e-02,  2.3783e-01, -1.8415e-01,\n",
       "        -2.6274e-01, -2.4807e-01, -2.6949e-01,  1.4914e-01, -1.9750e-01,\n",
       "         9.6772e-01,  8.4332e-02,  1.0363e-01,  1.2576e-02, -2.2948e-01,\n",
       "        -1.1784e-01,  9.3359e-03,  5.0288e-01,  7.6850e-01,  4.0112e-01,\n",
       "        -4.5457e-02, -1.4534e-01,  2.5848e-01,  4.5132e-01, -4.1712e-03,\n",
       "        -2.7386e-01,  2.5618e-01, -1.8180e-01, -2.8451e-01, -2.3413e-01,\n",
       "         2.1252e-02,  3.4913e-01, -1.8067e-01, -2.1376e-01,  9.8650e-01,\n",
       "        -4.4107e-01,  3.7352e-01,  1.8654e-01, -3.7053e-02, -1.8040e-01,\n",
       "        -8.8111e-01,  4.6106e-01, -1.3429e-01,  6.1358e-01, -3.9898e-01,\n",
       "        -3.2921e-01,  3.4685e-01,  6.1076e-02,  1.6279e-01,  2.2424e-01,\n",
       "         2.9324e-01,  2.6029e-02, -3.2399e-01,  3.8441e-01,  2.6564e-01,\n",
       "        -7.1761e-02,  1.5462e-01, -6.6879e-01, -2.0571e-01, -1.0013e-01,\n",
       "         5.8422e-01, -6.8078e-02, -7.1236e-02, -1.1905e-01, -3.7294e-01,\n",
       "        -6.2979e-02,  4.0410e-01,  7.1038e-01, -5.0853e-02,  6.1323e-02,\n",
       "         2.0109e-01, -3.5131e-01, -1.2745e-02,  2.2342e-01,  2.6733e-01,\n",
       "        -1.1622e-01, -3.1852e-02,  5.1675e-01, -1.9420e-01,  4.2959e-01,\n",
       "        -1.3590e-01, -8.8158e-02,  1.0419e-01, -7.1527e-03,  1.3783e-01,\n",
       "        -2.5816e-01,  4.2070e-01,  3.2537e-01, -2.3497e-01,  2.4386e-01,\n",
       "         2.0049e-01,  5.8766e-02,  4.7090e-02,  1.3561e-01,  2.0401e-01,\n",
       "         6.5540e-03,  1.2348e-01, -2.2109e-01, -2.6404e-01, -3.0941e-01,\n",
       "        -1.0101e-01, -2.7811e-01, -5.6633e-01,  3.4896e-01,  1.9106e-01,\n",
       "         4.0698e-01, -5.8428e-01,  4.1291e-01,  2.7370e-01, -3.9787e-01,\n",
       "        -3.1072e-01,  9.0169e-02,  3.1425e-01, -5.1760e-01,  7.6444e-02,\n",
       "        -1.8985e-01,  2.5566e-02, -9.3965e-02, -4.3337e-02, -2.6947e+00,\n",
       "        -1.3591e-01,  2.0581e-02, -1.3857e-02, -1.9665e-01, -1.1123e-01,\n",
       "        -3.0069e-01,  4.4234e-01, -1.8951e-01,  2.8825e-01, -2.1343e-01,\n",
       "        -8.9844e-02,  2.8515e-01, -4.3214e-02,  1.9722e-01,  4.0769e-01,\n",
       "         1.0127e-01,  1.4557e-01,  5.3141e-02, -8.6166e-02, -1.6809e-02,\n",
       "        -1.6306e-01, -1.4680e-01,  1.7328e-01, -8.2974e-02, -2.0902e-01,\n",
       "         9.0557e-02,  8.6546e-02, -1.1931e-01,  1.5786e-01,  5.9783e-01,\n",
       "        -1.0498e-01,  1.8417e-01,  9.8438e-02,  1.4940e-01, -3.0176e-01,\n",
       "        -2.5345e-01,  3.1119e-01,  4.9875e-02, -4.1381e-01, -4.6658e-01,\n",
       "        -7.8059e-02,  5.4129e-01,  8.9759e-02, -3.1766e-01,  2.1013e-01,\n",
       "        -2.3705e-01, -2.4798e-01, -4.6771e-01, -2.1807e-01, -3.5934e-01,\n",
       "         2.5933e-01, -3.4218e-01,  3.8913e-02, -2.0949e-01, -8.3503e-02,\n",
       "        -2.6685e-01, -7.7295e-02, -3.5916e-01,  8.3617e-02,  3.8130e-01,\n",
       "         3.2644e-01, -4.2112e-01,  3.3763e-01, -1.5886e-01,  4.6525e-01,\n",
       "        -4.8868e-02,  2.8235e-01, -2.3650e-01, -3.6407e-01, -2.6735e-02,\n",
       "        -2.0040e-01, -5.4365e-01, -4.5827e-02, -3.5582e-02, -5.1425e-02])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang.vocab.vectors[inp_lang.vocab.stoi[\"hello\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset.QGenDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  43498\n",
      "Load Failure :  5945\n"
     ]
    }
   ],
   "source": [
    "x._fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  43498\n",
      "Load Failure :  5945\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# from torchtext test cases\n",
    "import spacy\n",
    "from torchtext.data import Field, BucketIterator,TabularDataset\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import QGenDataset\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "inp_lang = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>',unk_token='<unk>')\n",
    "opt_lang = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>',unk_token='<unk>')\n",
    "\n",
    "dataset=QGenDataset()\n",
    "dataset._fetch_data(normalize=True)\n",
    "dataset.to_csv()\n",
    "\n",
    "data_fields = [('ans', inp_lang), ('que', opt_lang)]\n",
    "train,val = TabularDataset.splits(path='./.data/', train='train.csv', validation='val.csv', format='csv', fields=data_fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequential',\n",
       " 'use_vocab',\n",
       " 'init_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'fix_length',\n",
       " 'dtype',\n",
       " 'preprocessing',\n",
       " 'postprocessing',\n",
       " 'lower',\n",
       " 'tokenizer_args',\n",
       " 'tokenize',\n",
       " 'include_lengths',\n",
       " 'batch_first',\n",
       " 'pad_token',\n",
       " 'pad_first',\n",
       " 'truncate_first',\n",
       " 'stop_words',\n",
       " 'is_target',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " 'vocab_cls',\n",
       " 'dtypes',\n",
       " 'ignore',\n",
       " '__init__',\n",
       " '__getstate__',\n",
       " '__setstate__',\n",
       " '__hash__',\n",
       " '__eq__',\n",
       " 'preprocess',\n",
       " 'process',\n",
       " 'pad',\n",
       " 'build_vocab',\n",
       " 'numericalize',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__repr__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__setattr__',\n",
       " '__delattr__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__new__',\n",
       " '__reduce_ex__',\n",
       " '__reduce__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.fields[\"ans\"].__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "batch_size=12\n",
    "device=0\n",
    "val_iter = BucketIterator(val, batch_size=batch_size, \\\n",
    "            device=device, repeat=False , sort_key=lambda x: len(x.que), shuffle=True)\n",
    "train_iter = BucketIterator(train, batch_size=batch_size, \\\n",
    "            device=device, repeat=False , sort_key=lambda x: len(x.que), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14607"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_iter.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  0\n",
      "Load Failure :  43498\n",
      "Load Failure :  5945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "a,b,c,d= dataset.load_question_dataset(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74034"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.vocab.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
      "100%|█████████▉| 399295/400000 [00:50<00:00, 10339.40it/s]"
     ]
    }
   ],
   "source": [
    "c.vocab.load_vectors('glove.6B.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74034"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
